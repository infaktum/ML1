{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82444890-4c93-4667-ad14-091bcba2eb79",
   "metadata": {},
   "source": [
    "# Begriffe 1: Der Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b01cfb-77f6-4a16-a91a-4eaf7b44ca4a",
   "metadata": {},
   "source": [
    "### Integrationskonstante"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc39955c-d76c-486e-a831-d106fdea520e",
   "metadata": {},
   "source": [
    "Bereits bei der Linearen Regression hatten wir festgestellt, dass wir neben dem Parameter der Steigung $m$ noch einen zweiten Parameter $b$ benötigten, obwohl es sich eigentlich um ein eindimensionales Problem handelt. Auch bei der  Polynomialen Regression tauchte eine Konstante $a_0$ auf. Diese Konstanten waren notwendig, denn die übrigen Parameter sorgten nur dafür, dass die Ausgleichsgerade bzw. das Ausgleichspolynom die *Änderungsrate zwischen den Messwerten* berücksichtigt, aber nicht deren absolute Lage; d.h. die approximierende Kurve hat zwar die richtige Gestalt, aber nicht die richtige Position und muss noch vertikal verschoben werden. Dazu ist ein konstanter Faktor notwendig, den wir in der Schulmathematik als **Integrationskonstante** kennengelernt haben. Ohne diese Konstante könnten wir nur Funktionen approximieren, die im Nullpunkt den Wert Null haben.\n",
    "\n",
    "Auch im Deep Learning taucht diese Konstante auf; sie wird dann der **Bias** genannt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c61c87-e83b-40bf-a813-a396c474928f",
   "metadata": {},
   "source": [
    "### Der Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610c5f46-294e-4336-be9f-3059393e06e5",
   "metadata": {},
   "source": [
    "Der Bias ist für neuronale Netze unerlässlich; nur auf diese Weise lässt sich bei Eingangssignalen 0 ein von 0 verschiedenes Ausgangssignal erzielen. Der Bias entspricht bei der Linearen und Polynomialen Regression dem konstanten Koeffizienten $a_0$. \n",
    "\n",
    "Bereits beim Perzeptron haben wir den Bias $b$ kennengelernt. Dort wurde er neben den beiden Gewichten $w_1,w_2$ eingeführt, um einen von 0 verschiedenen Ausgang zu erzielen, obwohl beide Eingänge 0 sind. Dies führte zu der Lernregel\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "w_{1,2} &\\rightarrow w_{1,2} + \\alpha (y - o) x_{1,2}\\\\\n",
    "b &\\rightarrow b + \\alpha (y - o )\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Wir brauchten also zwei Anweisungen, um die Gewichte und den Bias anzupassen. \n",
    "\n",
    "Es ist aber lästig, ständig zwei Regeln zu entwickeln, für $N$ Gewichte und den Bias. Mit einem einfachen Trick lässt sich das problem lösen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16f7bc9-cd7c-46f9-9ac2-7c79dbe22737",
   "metadata": {},
   "source": [
    "### Der verschwundene Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c53f3c1-cf77-48c2-9fe9-f0798f7f0e32",
   "metadata": {},
   "source": [
    "Wir zeigen das Vorgehen am Beispiel des Perzeptrons. Wir führen ein weiteres (\"virtuelles\") Eingangsneuron ein, an dem immer der Wert $1$ anliegt. Dieses Neuron bekommt den Index 0. Damit haben wir nun die Eingangsneuronen $x_0,x_1,x_2$. Betrachten wir nun $b$ als Gewicht dieses fiktiven Eingangsneurons und bezeichnen es mit $w_0$, so erhalten wir folgendes KNN:\n",
    "\n",
    "![Das Perzeptron mit virtuellem Input](images/Perzeptron2.png)\n",
    "\n",
    "Unsere Verlustfunktion lässt sich nun einfach schreiben als\n",
    "\n",
    "$$L(w_k) = \\sum_{k=0}^2 (w_k x_k - y_k)^2$$\n",
    "\n",
    "und unser Lernalgorithmus wird einfach zu\n",
    "\n",
    "$$w_k \\rightarrow w_k + \\alpha (y - o) x_k, \\;\\;\\;k=0,1,2.$$\n",
    "\n",
    "Dieser kleine Kniff macht die Formeln sehr übersichtlich. Man muss sich lediglich merken, dass das 0. Neuron immer den Wert 1 hat. Wir werden dies in Zukunft bei Bedarf verwenden, wie es auch in der Fachliteratur häufig gemacht wird."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b43160-ec7c-4b8d-9456-4683080f2bd0",
   "metadata": {},
   "source": [
    "### Implementierung des Perzeptrons, 2.Version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b748f7-aef2-42b9-98d5-e0643c285cf7",
   "metadata": {},
   "source": [
    "Wir wollen den Trick mit dem verschwundenen Bias direkt einmal bei der Implementierung des Perzeptrons ausprobieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69881d32-93f3-425a-9908-a52a1e0204e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784a381f-cc69-4d6a-965c-5040b0950f1e",
   "metadata": {},
   "source": [
    "#### Die Funktionen vereinfachen sich noch einmal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53f65697-37ce-4f90-a934-38610f5450ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output(x,w):\n",
    "    return 1 if np.sum(w * x) > 0 else 0\n",
    "\n",
    "def lernschritt(x,y,w,alpha): \n",
    "    return w + alpha * (y - output(x,w)) * x\n",
    "\n",
    "def training(testdaten,w,alpha=0.01,maxiter=1000):\n",
    "    for i in np.random.randint(4,size=maxiter):\n",
    "        w = lernschritt(testdaten[i,:-1],testdaten[i,-1],w,alpha)              \n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a7883d-a780-4d76-818e-6412d25ae77b",
   "metadata": {},
   "source": [
    "#### Training und Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d28d69f-a7b4-4033-8c14-0bb66b7905b8",
   "metadata": {},
   "source": [
    "##### Anpassung der Testdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62d7d695-66d4-44b7-b9d5-35bec8615c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_und   = np.array([[0,0,0],[0,1,0],[1,0,0],[1,1,1]])\n",
    "test_nund  = np.array([[0,0,1],[0,1,1],[1,0,1],[1,1,0]])\n",
    "test_oder  = np.array([[0,0,0],[0,1,1],[1,0,1],[1,1,1]])\n",
    "test_xoder = np.array([[0,0,0],[0,1,1],[1,0,1],[1,1,0]])\n",
    "\n",
    "ones = np.array([[1] for _ in range(len(test_nund))])\n",
    "\n",
    "test_und   = np.append(ones,test_und,axis=1)\n",
    "test_nund  = np.append(ones,test_nund,axis=1)\n",
    "test_oder  = np.append(ones,test_oder,axis=1)\n",
    "test_xoder = np.append(ones,test_xoder,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ea4e1f9-3daa-44dd-92d4-21378d8e3233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainiert und testet ein Perzeptron\n",
    "\n",
    "def tut(name,daten):\n",
    "\n",
    "    w = training(daten,np.zeros(len(daten) - 1))\n",
    "    \n",
    "    print(f'{name}: ')\n",
    "    print(f'Gewichte = {w}\\n')\n",
    "    for x in daten:\n",
    "        o = output(x[:-1],w)\n",
    "        check = '\\u2713' if o == x[3] else '\\u21af'\n",
    "        print(f'{x[1]},{x[2]} -> {x[3]}, Output: {o}   {check}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d693b5bf-0a37-419a-8399-7f9df14e1b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UND-Verknüpfung: \n",
      "Gewichte = [-0.01  0.01  0.01]\n",
      "\n",
      "0,0 -> 0, Output: 0   ✓\n",
      "0,1 -> 0, Output: 0   ✓\n",
      "1,0 -> 0, Output: 0   ✓\n",
      "1,1 -> 1, Output: 1   ✓\n",
      "\n",
      "\n",
      "NICHT-UND-Verknüpfung: \n",
      "Gewichte = [ 0.03 -0.02 -0.01]\n",
      "\n",
      "0,0 -> 1, Output: 1   ✓\n",
      "0,1 -> 1, Output: 1   ✓\n",
      "1,0 -> 1, Output: 1   ✓\n",
      "1,1 -> 0, Output: 0   ✓\n",
      "\n",
      "\n",
      "ODER-Verknüpfung: \n",
      "Gewichte = [0.   0.01 0.01]\n",
      "\n",
      "0,0 -> 0, Output: 0   ✓\n",
      "0,1 -> 1, Output: 1   ✓\n",
      "1,0 -> 1, Output: 1   ✓\n",
      "1,1 -> 1, Output: 1   ✓\n",
      "\n",
      "\n",
      "XODER-Verknüpfung: \n",
      "Gewichte = [0.01 0.   0.  ]\n",
      "\n",
      "0,0 -> 0, Output: 1   ↯\n",
      "0,1 -> 1, Output: 1   ✓\n",
      "1,0 -> 1, Output: 1   ✓\n",
      "1,1 -> 0, Output: 1   ↯\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tut('UND-Verknüpfung',test_und)\n",
    "tut('NICHT-UND-Verknüpfung',test_nund)\n",
    "tut('ODER-Verknüpfung',test_oder)\n",
    "tut('XODER-Verknüpfung',test_xoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57816d00-c27a-4f43-8909-3d41ca7fcf74",
   "metadata": {},
   "source": [
    "### Zusammenfassung\n",
    "\n",
    "1. Der Bias ist im Deep Learning ein wichtiges Element, da durch ihn einige Aufgaben erst gelöst werden können.\n",
    "2. Indem der Bias als Gewicht eines zusätzlichen fiktiven Eingabeneurons betrachtet wird, vereinfachen sich viele Rechnungen. Allerdings müssen die Testdaten angepasst werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6284914b-1beb-4d7e-8b2f-b0b4d1adda0b",
   "metadata": {},
   "source": [
    "### Links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fca910c-490e-40b4-b835-f03ff063326c",
   "metadata": {},
   "source": [
    "[1] [Bias in der Wikipedia](https://de.wikipedia.org/wiki/K%C3%BCnstliches_Neuron#Bias)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
