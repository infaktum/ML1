{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Iris-DB mit MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Beim Iris-Datensatz haben wir gesehen, dass die zusammengehörenden Datensätze im vierdimensionalen Raum nahe beieinanderliegen, also __Cluster__ bilden. Durch einfaches Betrachten verschiedener Darstellungen der Daten konnten wir diese Cluster leicht trennen. Der \"natürliche\" Ansatz, dies zu autimatisieren, war die Methode des __k-means__: Wir suchten k = 3 verschiedene Mittelwerte, um die sich diese Cluster bilden.\n",
    "\n",
    "Obwohl das Problem dadurch hinreichend gut gelöst ist, wollen wir nun versuchen, das Problem mit einem einfachen Neuronalen Netzwerk, genauer einem Multi-Layered Perceptron, zu lösen. Diese Lösung ist zwar schon fast überdimensioniert, aber da das Problem noch übersichtlich ist, können wir die Wirkungsweise neuronaler Netzwerke hier gut erkennen, bevor wir zu größeren Problemen (was Daten-Dimension und -Volumen anbetrifft) übergehen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Laden der Iris-Daten\n",
    "\n",
    "Wir lesen zunächst die Daten aus der Iris-Datenbank ein und teilen diese dann in eine Menge von Trainingsdaten auf, mit denen das Netzwerk trainiert wird, und eine Menge von Testdaten, mit denen wir dann das Ergebnis überprüfen.\n",
    "\n",
    "Zuvor skalieren wir aber alle numerischen Werte auf den Bereich zwischen 0 und 1. Das generische MLP, dass wir verwenden, erwartet die Daten in diesem Format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = pd.read_csv('iris.csv')\n",
    "iris['CName'] = pd.factorize(iris['Name'])[0]\n",
    "\n",
    "X, y, namen  = iris.iloc[:, :-2].values, iris.iloc[:, -1].values, iris.iloc[:, -2].values\n",
    "\n",
    "scaler = MinMaxScaler(copy = False)\n",
    "scaler.fit_transform(X );\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4576)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Erzeugung des MLP\n",
    "\n",
    "Wir erzeugen unser MLP mit \n",
    "\n",
    "* 4 Neuronen in der Eingabeschicht (vier Features)\n",
    "* 3 Neuronen in der versteckten Schicht (?)\n",
    "* 3 Neuronen in der Ausgabeschicht (drei Iris-Sorten)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mlp import MLP\n",
    "\n",
    "mlp = MLP(4,3,3)\n",
    "\n",
    "mlp.fit(X_train,y_train,epochs = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test des Neuronalen Netzes anhand der Testdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction: \t[2 1 1 1 1 2 1 0 2 2 2 2 1 0 2 2 2 1 2 2 2 0 0 2 1 1 2 2 1 2]\n",
      "Target: \t[2 1 1 1 1 2 1 0 2 2 2 2 1 0 2 2 2 1 2 2 2 0 0 2 1 1 2 2 1 2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "pred = np.array([mlp.predict(x) for x in X_test])\n",
    "\n",
    "print(f'prediction: \\t{pred}\\nTarget: \\t{y_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie gut ist unser neuronales Netz? Dazu zählen wir die Treffer auf der gesamten Menge der Testdaten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genauigkeit: 100.0%\n"
     ]
    }
   ],
   "source": [
    "score, _ = mlp.score(X_test, y_test)\n",
    "print (f'Genauigkeit: {score:.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wir speichern die Gewichte des Netzwerks zur folgenden Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gewichte wurden gespeichert\n"
     ]
    }
   ],
   "source": [
    "mlp.save(\"iris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
