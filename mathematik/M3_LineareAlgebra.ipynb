{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b304aff-bcc0-4491-8a07-ddd03f9ab562",
   "metadata": {},
   "source": [
    "# Mathematik 3: Skalare, Vektoren, Matrizen, Tensoren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3591ebf0-93bd-4a7a-b3c5-d6f34432778c",
   "metadata": {},
   "source": [
    "Eine Reihe von Klassikern der Fachliteratur zur Programmierung tragen den Namen **Algorithmen und Datenstrukturen**. Die Autoren geben Datenstrukturen dieselbe Bedeutung wie den Algorithmen, an die man zuerst bei Programmierung denkt. In der Welt der Mathematik stellen (Daten-)Strukturen die Grundlage dar, auf der alles aufgebaut wird. Die Datenstrukturen, die bei den KNN verwendet wird, stammen aus dem Bereich der **Linearen Algebra**, der wohl (und glücklicherweise) einfachsten Disziplin der *Höheren Mathematik*. Wir beschreiben auf dieser Seite die wichtigsten Begriffe aus diesem Teil der Mathematik.\n",
    "\n",
    "Auch wenn man nicht tief in die für viele mystisch/mysteriöse Welt eintauchen möchte, so sind vor allem die kompakte und einheitliche Schreibweise und die eindeutigen Begriffsbildungen für das Verständnis nicht nur Künstlicher Neuronaler Netze hilfreich. Der praktische Umgang mit diesen Strukturen und den Operationen darauf sind der Schlüssel zum Deep Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060b1231-782d-4dc5-9d66-b5a97113cae2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Skalare und Vektoren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d164a2c-4202-430d-9004-cdeba3b3fb8c",
   "metadata": {},
   "source": [
    "\n",
    "   \n",
    "Ein **Skalar** ist in der Linearen Algebra einfach eine reelle Zahl, und man verwendet den Begriff nur, um eine **skalare Größe** von einem Vektor zu unterscheiden. Mit Skalaren zu rechnen haben wir in der Grundschule gelernt.\n",
    "\n",
    "Ein **Vektor** fasst mehrere Skalare, die in einem Zusammenhang stehen, zu einer Einheit zusammen. Dies kennen wir aus Koordinaten eines Punktes in einer Ebene, dessen x- und y-Koordinate zusammen den **Ortsvektor** des Punktes bilden.(Auch das haben wir auf der Schule gelernt.) Die einzelnen Zahlen, aus den der Vektor gebildet wird, nennt man die **Komponenten** des Vektors, und die Anzahl der Komponenten in einem Vektor wird die **Dimenson** genannt. Um Vektoren von Skalaren zu unterscheiden, zeichnet man oft einen kleinen Pfeil über den Namen des Vektors, also etwa $\\vec{v}$. Spannend ist nun, dass man mit Vektoren teilweise alles machen kann wie mit Skalaren: Man kann sie addieren, subtrahieren und multiplizieren (aber nich dividieren), und es gibt sogar mehrere Möglichkeiten der Multiplikation.\n",
    "\n",
    "\n",
    "Wir haben bereits eine andere Sorte von Vektoren kennengelernt, an die man nicht sofort denkt: Auch **Polynome können als Vektoren aufgefasst werden**! Denn jedes Polynom $P = a_0 + a_1 x + a_2 x^2 + \\ldots + a_N x^N$ ist durch seine Koeffizienten $a_k$ bestimmt, und wir können diese Koeffizienten als Vektor zusammenfassen. Somit ist unser Polynom $P$ ein Vektor der Form $ P=\\begin{pmatrix} a_0 \\\\ a_1 \\\\ \\dots \\\\ a_N \\end{pmatrix}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085a55a1-69e4-4d26-8541-08aaf4e853f0",
   "metadata": {},
   "source": [
    "### Rechnen mit Vektoren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727200a8-f7a7-45db-a4a8-fc5075910460",
   "metadata": {},
   "source": [
    "Wir zeigen an den Beispielen der Ortsvektoren und Polynomen, wie man mit Vektoren rechnen kann.\n",
    "\n",
    "#### Die Addition\n",
    "\n",
    "Zwei Ortsvektoren $\\vec{v} = \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix}$ und $\\vec{w} = \\begin{pmatrix} w_1 \\\\ w_2 \\end{pmatrix}$ werden addiert, indem man einfach die Komponenten addiert:  $\\vec{v} + \\vec{w} = \\begin{pmatrix} v_1 + w_1 \\\\ v_2 + w_2\\end{pmatrix}$.\n",
    "\n",
    "Für Polynome sieht dies so aus:\n",
    "\n",
    "$P(x) + Q(x) = (a_0 + a_1x+ a_2 x^2 + \\ldots + a_N x^N) + (b_0 + b_1x+ b_2 x^2 + \\ldots + b_N x^N) = (a_0 + b_0) + (a_1 + b_1)x+ (a_2 + b_2) x^2 + \\ldots + (a_N + b_N) x^N$\n",
    "\n",
    "\n",
    "#### Die Multiplikation mit einem Skalar\n",
    "\n",
    "Einen Ortsvektor $\\vec{v} = \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix}$ kann man mit einer Zahl $k$ multiplizieren, indem man jede Komponente mit dieser Zahl multipliziert: $k \\vec{v} = \\begin{pmatrix} k v_1 \\\\ k v_2 \\end{pmatrix}$\n",
    "\n",
    "Und bei Polynomen:\n",
    "\n",
    "$k P(x) = (k a_0) + (k a_1) x+ (k a_2) x^2 + \\ldots + (k a_N) x^N $\n",
    "\n",
    "#### Multiplikation zweier Vektoren\n",
    "\n",
    "Für die Multiplikation zweier Vektoren gibt es mehrere Möglichkeiten, zu denen allerdings die naheliegendste, nämlich einfach die komponetenweise Multiplikation, *nicht* gehört.\n",
    "\n",
    "##### Das Skalarprodukt\n",
    "\n",
    "Das wichtigste Produkt zweier Vektoren ist das **Skalarprodukt**, das auch **Inneres Produkt** oder **Punktprodukt** (letzteres wegen der Schreibweise) genannt wird:\n",
    "\n",
    "$\\vec{v} \\cdot \\vec{w} = \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix} \\cdot \\begin{pmatrix} w_1 \\\\ v_2 \\end{pmatrix}= v_1 w_1 + v_2 w_2$\n",
    "\n",
    "In höheren Dimensionen lässt sich dies kompakt schreiben als $\\vec{v} \\cdot \\vec{w} = \\sum_{k=1}^N v_k w_k$. Es wird also zwar die komponentenweise Multiplikation durchgeführt, danach werden aber die Ergebnisse aufsummiert, wodurch ein Skalar enteht. *Das Skalarprodukt ist von fundamentaler Bedeutung*, auch für unsere KNN, so dass wir darüber noch ein eigenes Kapitel schreiben.\n",
    "\n",
    "Ein zweites Produkt zwischen Vektoren ist das **Vektorprodukt** oder **Kreuzprodukt** (wegen der Schreibweise $\\vec{v} \\times \\vec{w}$), das allerdings nur für dreidimensionale Vektoren definiert und vor allem für Geometrie und Physik bedeutsam ist. Für unsere Zwecke ist es uninteressant. Das Ergebnis dieses Produkts ist ein Vektor, der senkrecht auf beiden Vektoren steht.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3917524-f6d4-4b98-acf0-373c84b26a5d",
   "metadata": {},
   "source": [
    "### Matrizen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3074ef27-9c15-42e9-b8df-2f987fa175b7",
   "metadata": {},
   "source": [
    "Den nächsten Schritt in unserer Hierarchie stellen Matrizen dar. Eine **Matrix** ist ein zweidimensionales Zahhlenschema der Gestalt $M = \\begin{pmatrix} a_{11} & \\ldots a_{1n} \\\\  \\ldots \\\\ a_{m1} & \\ldots a_{mn} \\end{pmatrix}$. Man spricht hier von einer $m\\times n $-Matrix, und ist $m=n$, so heißt die Matrix **quadratisch**.\n",
    "\n",
    "Sind vektoren die datenstrukturen  der Linearen Algebra,so sind Matrizen die Operationen darauf. Ein Vektor lässt sich ämlich it einer Matrix multiplizieren, wodurch ein neuer Vektor entsteht (dazu müssen natürlich die Dimensionen \"passen\")\n",
    "\n",
    "$$ M \\cdot v =  \\begin{pmatrix} a_{11} & \\ldots a_{1n} \\\\  \\ldots \\\\ a_{m1} & \\ldots a_{mn} \\end{pmatrix} \n",
    "\\begin{pmatrix} v_1 \\\\  \\ldots \\\\ v_n \\end{pmatrix} = \n",
    "\\begin{pmatrix} \\sum_{k=1}^n a_{1k} v_k \\\\  \\ldots \\\\ \\sum_{k=1}^n a_{mk} v_k \\end{pmatrix}.$$\n",
    "\n",
    "Hinter den vielen Indizes versteckt sich folgendes Rechenschema: Man nimmt dier erste Zeile der Matrix und multipliziert die Komponenten mit den Komponeten des vektors. Die Summe bildet die erste Komponente des Ergebnis-Vektors. Dann geht es mit der zweiten Zeile weiter, bis alle Zeilen der Matrix abgearbeitet sind. Die $k.$ Komponente des Ergebnisvektors ist also das Skalarprodukt der $k.$ Zeile der Matrix mit dem Vektor $\\vec{v}$.\n",
    "\n",
    "Man kann Matrizen auch addieren (komponentenweise) und sogar multiplizieren, wenn die Dimensionen übereinstimmen. Wir werden bei den KNN auf Matrizen stoßen, wenn wir die Gewichte des Neuronalen Netzes untersuchen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a96f48c-75e4-40e4-a998-67c49187c453",
   "metadata": {},
   "source": [
    "### Tensoren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddbdf18-23e8-4b27-a10a-2c22075e5615",
   "metadata": {},
   "source": [
    "Natürlich kann man das Spiel weiter treiben und nach den zweidimsionalen Matrizen Zahlenschemata mit noch mehr Dimensionen betrachten. Jenseits der Matrizen spricht man ganz allgemein von **Tensoren**. Dabei sind Skalare, Vektoren und Matrizen ebenso Tensoren, mit der Dimension 0, 1 bzw. 2. Dabei spricht man bei Tensoren eher von **Stufen** als von Dimensionen. Jenseits der Matrizen lassen sich Tensoren nicht mehr so schön hinschreiben, aber in Computerprogrammen ist es egal, wie viele Indizes man verwendet. Ein Tensor 3. Stufe ist also eine Menge an Zahlen, die strukturiert in der Form $T = (t_{ijk})$ geschrieben werden können.\n",
    "\n",
    "Tensoren werden wir in der Einführung zu KNN nicht besprechen. Sie spielen aber in der Programmierung eine wichtige Rolle, und  Googles KI-Paket heißt nach diesen Objekten **Tensorflow** (auch Facebooks Torch basiert auf Tensoren.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233e78b0-0cd4-4646-949e-d9e6f2faeb6a",
   "metadata": {},
   "source": [
    "# Lineare Algebra mit Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e028a9df-b3e8-49a8-aaeb-63e2cf71dc49",
   "metadata": {},
   "source": [
    "Die vorgestellten Strukturen und Rechenoperationen lassen sich natürlich leicht implementieren. Wir demonstrieren hier einmal die Umsetzung in Python, einmal mit Schleifen, und dann mit dem Paket **NumPy**. Dabei hat Python gegenüber vielen anderen Sprachen wie Java, C++ und C# einige Vorteile, da es von Natur aus Listen gut unterstützt und etwa im Gegensatz zu Java auch *Operator Overloading*, was vieles lesbarer macht.\n",
    "\n",
    "In Python lässt sich viele mathematische Ausdrücke der Linearen Algebra sehr schreiben, da Datenstrukturen wie Vektoren und Matrizen einen großen Stellenwert in Python haben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad054766-36bc-4076-ad8f-23720b1de3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summe(v,w):\n",
    "    return [v+w for v,w in zip(v,w)]\n",
    "\n",
    "def produkt(k,v):\n",
    "    return [k * v for v in v]\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "def skalar_produkt(v,w):\n",
    "    return reduce((lambda x, y: x + y), [v*w for v,w in zip(v,w)])\n",
    "\n",
    "def produkt_matrix(m,v):\n",
    "    return [[m*v for m,v in zip(m,v)]  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "478d7039-88ad-4481-be22-2e7f7f87471d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v = [1, 2], \n",
      "w = [3, 4],\n",
      "M = [[1, 2], [3, 4]]\n",
      "\n",
      "v + w = [4, 6]\n",
      "k v = [3, 6]\n",
      "v . w = 11\n",
      "M v = [[[1, 2], [3, 4, 3, 4]]]\n"
     ]
    }
   ],
   "source": [
    "k = 3\n",
    "v = [1,2]\n",
    "w = [3,4]\n",
    "m = [[1,2],[3,4]]\n",
    "\n",
    "print(f'v = {v}, \\nw = {w},\\nM = {m}\\n')\n",
    "\n",
    "print(f'v + w = {summe(v,w)}')\n",
    "print(f'k v = {produkt(k,v)}')\n",
    "print(f'v . w = {skalar_produkt(v,w)}')\n",
    "print(f'M v = {produkt_matrix(m,v)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6015f7fb-d3f3-4330-b21f-d24af9bbb7c7",
   "metadata": {},
   "source": [
    "### Mit NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dc8447-7800-44fa-9ad6-1ea417c9bc75",
   "metadata": {},
   "source": [
    "Mit NumPy geht alles sehr viel schöner und vor allem schneller, was man erst bei sehr großen Datenmengen merkt. NumPy rechnet intern mit anderen (\"intelligenten\") Datenstrukturen, daher müssen alle Werte zuerst in ein internes Format umgewandelt werden, das **ndarray** (\"N-dimensionales Array\"). Danach sind all unsere Operatoren bereits vorhanden. Dank *Operator Overloading* brauchen wir keine Funktionen zu definieren, und NumPy weiß aufgrund der Datenstrukturen zu entscheiden, dass kv das Produkt einer  Konstanten mit einem Vektor ist, und Mv das entsprechende Produkt mit einer Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb6606f2-1717-4873-a277-1580ac6ccdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1972d0ea-35de-4f2c-8ce9-6daedfdc5f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v = [1 2], \n",
      "w = [3 4],\n",
      "M = [[1 2]\n",
      " [3 4]]\n",
      "\n",
      "v + w = [4 6]\n",
      "k v = [3 6]\n",
      "v . w = [3 8]\n",
      "M v = [[1 4]\n",
      " [3 8]]\n"
     ]
    }
   ],
   "source": [
    "k = 3\n",
    "v = np.array([1,2])\n",
    "w = np.array([3,4])\n",
    "m = np.array([[1,2],[3,4]])\n",
    "\n",
    "print(f'v = {v}, \\nw = {w},\\nM = {m}\\n')\n",
    "\n",
    "print(f'v + w = {v+w}')\n",
    "print(f'k v = {k*v}')\n",
    "print(f'v . w = {v*w}')\n",
    "print(f'M v = {m * v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37435739-d308-4076-86c5-cf927c938732",
   "metadata": {},
   "source": [
    "---\n",
    "Mit NumPy werden viele Aufgaben kinderleicht, und der Code ist wirklich leichter zu verstehen.\n",
    "\n",
    "Aber **Achtung**: Zu den einzelnen Operatoren gibt es auch andere Implementierung, etwa **dot** für das Skalarprodukt, die deutlich schneller sind als die Operationen in der \"einfachen\" Schreibweise. Wir werden sie auch an passenden Stellen bevorzugt einsetzen."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
