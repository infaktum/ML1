{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec961a80-f0c1-4ded-b893-f66053528a89",
   "metadata": {},
   "source": [
    "# Skalierung von Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb60de9-a6e5-4582-a0f1-e439c50fc2fe",
   "metadata": {},
   "source": [
    "Skalierung ist wohl die einfachste denkbare Transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25b08f94-94de-4f0e-85c9-534e49acbb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets as ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bce8e0-06df-4713-b635-400adaa80e4f",
   "metadata": {},
   "source": [
    "## Scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c068b7f-0fa5-4209-8435-f9f653560114",
   "metadata": {},
   "source": [
    "`Scikit-learn` bringt im Paket `sklearn.preprocessing` verschiedene Scaler mit, durch die man auf einfache Weise große Datenmengen anpassen kann."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd9473a-cabd-48ad-bfce-db574c2d39f6",
   "metadata": {},
   "source": [
    "### MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943dc5b6-d8b5-4bfc-88c3-c00b5ea15ee2",
   "metadata": {},
   "source": [
    "Nehmen wir an, uns liegt ein Datensatz vor, wie er etwa durch `make_blobs` zufällig erzeugt werden kann:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0592340-43dd-4cd0-b2ff-9bf4d035c87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unskalierte Daten:\n",
      "[[ 2.22925534 -5.62047603]\n",
      " [ 5.39460748 -8.420975  ]\n",
      " [ 2.50302812 -2.34142504]\n",
      " [-7.95680613  7.99785955]\n",
      " [ 7.12609218 -8.95760642]]\n"
     ]
    }
   ],
   "source": [
    "raw_data, _ = ds.make_blobs(n_samples = 5, n_features = 2)\n",
    "print(f'Unskalierte Daten:\\n{raw_data}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a7178c-3d0a-4a29-a664-11167bcd9daa",
   "metadata": {},
   "source": [
    "Die zweidimensionalen Daten sind gleichmäßig zufällig in einem durch `make_blobs` bestimmten Bereich verteilt; es gibt sowohl negative als auch positive Daten.\n",
    "\n",
    "Viele Verfahren erwarten jedoch als Eingabe Daten, die in einem vorgegebenen Bereich liegen, etwa im Bereich $[0,1]$. Es ist nun sehr leicht, die Daten mit Hilfe von `NumPy` zu skalieren, indem man Minimum und Maximum der Daten bestimmt und einen passenden Skalierungs- und Verschiebungsfakto bestimmt.\n",
    "\n",
    "Mit Hilfe des `MinMaxScalers` aus dem Paket `sklearn.preprocessing` geht das noch einfacher:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2d2916c7-a637-41cd-acd0-88dba270e0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skaliert:\n",
      "[[1.         1.        ]\n",
      " [0.04356402 0.6437776 ]\n",
      " [0.27636416 0.        ]\n",
      " [0.32603638 0.08608675]\n",
      " [0.         0.68388647]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(raw_data)\n",
    "\n",
    "print(f'Skalierte Daten:\\n{scaled_data}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdeb7607-3a3e-4a41-a84e-8918777f34ec",
   "metadata": {},
   "source": [
    "Mit Hilfe der Funktion `fit_transform` der Klasse `MinMaxScaler` werden die Daten standardmäßig auf den Bereich $[0,1]$ skaliert, und zwar abhängig voneinander in beiden Dimensionen (Spalten).\n",
    "\n",
    "Mit der Methode `fit` kann ein Scaler zunächt an vorhandene Daten angepasst werden, bevor mit `transform` die eigentliche Transformation durchgeführt wird. Das ist wichtig, wenn mehrere Datensätze mit derselben Skalierung angepasst werden müssen. `fit_transform` macht dies in einem Durchgang, wenn das nicht der Fall ist.\n",
    "\n",
    "Natürlich sind auch andere Wertebereiche vorstellbar, und mit `MinMaxScaler` lässt sich wieder entsprechend skalieren, z.B. auf den Bereich $[-3,11]$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "27eae294-509c-4d4f-9231-dad3b976d089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11.         11.        ]\n",
      " [-2.39010372  6.01288637]\n",
      " [ 0.86909818 -3.        ]\n",
      " [ 1.56450934 -1.79478543]\n",
      " [-3.          6.57441059]]\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler((-3,11))\n",
    "print(scaler.fit_transform(raw_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d3d1ae-06db-4936-8705-6a1be99eef59",
   "metadata": {},
   "source": [
    "### StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31db63d-95a1-4c47-89ed-8460aaff8e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224d04fe-0d5e-4f7c-9e5b-2379350b4307",
   "metadata": {},
   "source": [
    "## Zusammenfassung\n",
    "\n",
    "Skalierung ist programmiertechnisch keine große Sache und könnte auch einfach mit `NumPy` erledigt werden. Der Vorteil der Scaler und anderer Transformatoren aus `Scikit-learn` besteht in der einheitlichen API. Da alle Transformer dieselben Schnittstellen besitzen wie die eigentlichen KI-Verfahren auch (insbesondere `fit`), können sie zu `Pipelines` zusammgesteckt werden, die dann nacheinander alle Operationen automatisiert ausführen. Dazu später mehr im Teil zur __Automatisierung__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fd1cfb-97e7-4073-b278-e03079e347a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
