{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3917524-f6d4-4b98-acf0-373c84b26a5d",
   "metadata": {},
   "source": [
    "## Transformation von Vektoren: Matrizen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3074ef27-9c15-42e9-b8df-2f987fa175b7",
   "metadata": {},
   "source": [
    "Den nächsten Schritt in unserer Hierarchie stellen Matrizen dar. Eine **Matrix** ist ein zweidimensionales Zahhlenschema der Gestalt $M = \\begin{pmatrix} a_{11} & \\ldots a_{1n} \\\\  \\ldots \\\\ a_{m1} & \\ldots a_{mn} \\end{pmatrix}$. Man spricht hier von einer $m\\times n $-Matrix, und ist $m=n$, so heißt die Matrix **quadratisch**.\n",
    "\n",
    "Sind Vektoren die Datenstrukturen  der Linearen Algebra, so sind Matrizen die Operationen darauf. Ein Vektor lässt sich mit einer Matrix multiplizieren, wodurch ein neuer Vektor entsteht (dazu müssen natürlich die Dimensionen \"passen\")\n",
    "\n",
    "$$ M \\cdot v =  \\begin{pmatrix} a_{11} & \\ldots a_{1n} \\\\  \\ldots \\\\ a_{m1} & \\ldots a_{mn} \\end{pmatrix} \n",
    "\\begin{pmatrix} v_1 \\\\  \\ldots \\\\ v_n \\end{pmatrix} = \n",
    "\\begin{pmatrix} \\sum_{k=1}^n a_{1k} v_k \\\\  \\ldots \\\\ \\sum_{k=1}^n a_{mk} v_k \\end{pmatrix}.$$\n",
    "\n",
    "Hinter den vielen Indizes versteckt sich folgendes Rechenschema: Man nimmt dier erste Zeile der Matrix und multipliziert die Komponenten mit den Komponeten des vektors. Die Summe bildet die erste Komponente des Ergebnis-Vektors. Dann geht es mit der zweiten Zeile weiter, bis alle Zeilen der Matrix abgearbeitet sind. Die $k.$ Komponente des Ergebnisvektors ist also das Skalarprodukt der $k.$ Zeile der Matrix $M$ mit dem Vektor $\\v$.\n",
    "\n",
    "Man kann Matrizen auch addieren (komponentenweise) und sogar multiplizieren, wenn die Dimensionen übereinstimmen. Wir werden bei den KNN auf Matrizen stoßen, wenn wir die Gewichte des Neuronalen Netzes untersuchen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f7af59-ae79-46ff-9d43-d2a379226daf",
   "metadata": {},
   "source": [
    "## Matrizen in der KI\n",
    "\n",
    "Bei mehrschichtigen Neuronalen Netzen wie dem MLP (Multi-Layered Perceptron) sind die Neuronen der verschiedenen Schichten miteinander verknüpft, wobei jede Verknüpfung mit einem Gewicht versehen sind. Wenn wie beim MLP sämtliche $M$ Neuronen einer Schicht mit sämtlichen $N$ Neuronen der folgenden Schicht verbunden, so können die Gewichte als eine $N \\times M$-Matrix betrachtet werden. \n",
    "\n",
    "![Gewichtsmatrix](gewichte.png)\n",
    "\n",
    "In diesen Gewichtsmatrizen ist die Information des Neuronalen Netzwerks gespeichert. Wird am MLP ein Eingangssignal angelegt (ein $M$-dimensionaler Raum), so wird dieses Signal durch die Matrix transfomiert und in den $N$-dimensionalen Raum der nächsten (versteckten) Schicht abgebildet.\n",
    "\n",
    "Welche Bedeutung hat der $N$-dimensionale Raum der Vektoren der verdeckten Schicht? Wie wird das Wissen in den Gewichten gespeichert? Klar ist dieser Mechanismus nicht. Neuronale Netze sind ungeheuer leistungsfähig, aber schwer zu steuern, denn es gibt viele Parameter: Anzahl der Schihten und der Neuronen in jeder Schicht sowie die Lernrate. Auch die Reihenfolge, in der die Lerndaten präsentiert werden, spielt eine Rolle. Das Thema ist äußerst komplex und Gegenstand der Forschung. \n",
    "\n",
    "### Beispiel: MLP zur Ziffernerkennung  \n",
    "\r\n",
    "Angenommen, ein MLP soll handgeschriebene Zahlen erkennen. Die ersten Schichten könnten lernen: \n",
    " \r\n",
    "- **Kanten und Striche**:  z.B. vertikale und horizontale Linien \r\n",
    "- **Zahlenfragment:e**: z.B. geschlossene Kreise für die „0“\r\n",
    "- **Komplette Zahl:e**: z.B. Erkennung einer „8“ als Kombination aus Kreisen und Strichen \r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a96f48c-75e4-40e4-a998-67c49187c453",
   "metadata": {},
   "source": [
    "### Tensoren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddbdf18-23e8-4b27-a10a-2c22075e5615",
   "metadata": {},
   "source": [
    "Bei Zahlenschemas höherer Dimension nennt man ganz allgemein **Tensoren**. Dabei sind Skalare, Vektoren und Matrizen ebenso Tensoren, mit der Dimension 0, 1 bzw. 2. Dabei spricht man bei Tensoren eher von **Stufen** als von Dimensionen. Jenseits der Matrizen lassen sich Tensoren nicht mehr so schön hinschreiben, aber in Computerprogrammen ist es egal, wie viele Indizes man verwendet. Ein Tensor 3. Stufe ist also eine Menge an Zahlen, die strukturiert in der Form $T = (t_{ijk})$ geschrieben werden können. Googles KI-Paket heißt nach diesen Objekten **Tensorflow** (auch Facebooks Torch basiert auf Tensoren.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655c7554-0fae-44f9-ba53-53b9a3d63d0a",
   "metadata": {},
   "source": [
    "## Matrizen in Numpy\n",
    "\n",
    "Numpy unterstützt natürlich auch das Rechnen mit Matrizen. Für uns ist vor allen Dingen die Multiplikation eine Matrix $M$ mit einem Vektor $V$ interessant. Wie wir gesehen haben, entspricht dies\n",
    "\n",
    "* der Abbildung zwischen zwei (ggf. verschieden-dimensionalen) Räumen\n",
    "* der Verarbeitung von Input-Vektoren (und auch von Zwischenergebnissen) in einem Neuronalen Netz.\n",
    "\n",
    "Allerdings gibt es eine Besonderheit, auf die wir jetzt eingehen müssen.\n",
    "\n",
    "### Zeilen- und Spaltenvektoren\n",
    "\n",
    "Wir haben Vektoren bislang etwa so geschrieben: $ v = (v_1, v_2, ... , v_N) $. Mathematisch korrekt ist aber diese Form:\n",
    "$$ v=  \\begin{pmatrix} v_1 \\\\ v_2 \\\\ \\cdots \\\\ v_N\\end{pmatrix} $$\n",
    "\n",
    "Das nennt man einen _Spaltenvektor_, im Gegensatz zum platzsparenden _Zeilenvektor_. So exakt wollen wir nicht sein, aber für das Rechnen mit Numpy müssen wir eigentlich alle Vektoren als Spaltenvektoren schreiben. Die (mathematische) Operation, die einen Vektor (und auch eine Matrix) \"umklappt\" wird __Transponieren__ genannt. Numpy hat dafür auch eine Operation namens `transpose`.\n",
    "\n",
    "Versuchen wir einmal, eine Matrix und einen Vektor mit `Numpy`zu multiplizieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70d2ae64-bcb3-430f-9e07-7647b64ef3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14]\n",
      " [32]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "v = np.array([1,2,3],ndmin=2)\n",
    "M = np.array([[1,2,3],[4,5,6]])\n",
    "\n",
    "w = (M @ v.T)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d65b6d-3cf3-4ffe-b876-027da9418eaa",
   "metadata": {},
   "source": [
    "Dieses Vorgehen ist äquivalent zu der Form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a44f0335-5f86-4581-88de-871b2c198b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14]\n",
      " [32]]\n"
     ]
    }
   ],
   "source": [
    "w = np.dot(M, np.transpose(v))\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67f4121-6e46-4f78-9c64-e792dcc393f8",
   "metadata": {},
   "source": [
    "$w$ ist also das Ergebnis aus der Multiplikation der Matrix $M$ mit dem transponierten Vektor $v$. Wir werden bei der Arbeit mit den Vektoren in den Neuronalen Netzen die betreffenden Vektoren rechtzeitig transponieren müssen, damit \"alles klappt\".\n",
    "\n",
    "Das mag alles etwas verwirrend sein, aber es wir etwas einfacher, wenn man sich einen Zeilenvektor als Matrix mit nur einer Zeile vorstellt, und entsprechend einen Spaltenvektor als Matrix mit nur einer Spalte. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3d2768-81bd-46d6-be3c-369252e2e134",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
